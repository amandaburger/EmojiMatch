{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "import operator\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import cv2\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image data...\n",
      "  loaded data/train.                      \n",
      "  loaded data/test.                      \n"
     ]
    }
   ],
   "source": [
    "#main emotions are dataset comes with\n",
    "emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "\n",
    "#load data into our notebook\n",
    "def load_emotions_data(path):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        print(f\"\\r  {path}/{emotion}... ({i+1}/{len(emotions)})          \", end='')\n",
    "        for filename in glob.glob(f\"{path}/{emotion}/*.jpg\"):\n",
    "            img = Image.open(filename)\n",
    "            img = np.array(img.getdata()).reshape(48, 48) / 255\n",
    "            x.append(img)\n",
    "            y_onehot = np.zeros(len(emotions))\n",
    "            y_onehot[i] = 1\n",
    "            y.append(y_onehot)\n",
    "    print(f\"\\r  loaded {path}.                      \")\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "print(\"Loading image data...\")\n",
    "x_train, y_train = load_emotions_data(\"data/train\")\n",
    "x_test, y_test = load_emotions_data(\"data/test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#OUR MODEL\n",
    "batch_size = 128\n",
    "input_shape = (48, 48, 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=5, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(len(emotions), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#train model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 15:19:18.653907: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#model = tf.keras.models.load_model('saved_model/saved_model.pb')\n",
    "batch_size = 128\n",
    "input_shape = (48, 48, 1)\n",
    "model = tf.keras.models.load_model(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/57 [==============>...............] - ETA: 0s - loss: 1.0611 - accuracy: 0.5814"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fg/f4khpkkj039ffhb1np84vb7m0000gn/T/ipykernel_1669/578954008.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#display confusion matrix to demonstrate accuracy\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    ax  = fig.add_subplot(111)\n",
    "    matrix = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.RdYlGn)\n",
    "    # fig.colorbar(matrix) \n",
    "    for i in range(0,len(emotions)):\n",
    "        for j in range(0,len(emotions)):  \n",
    "            ax.text(j,i,np.round(cm[i,j],2),va='center', ha='center')\n",
    "    ticks = np.arange(len(emotions))\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(emotions, rotation=45)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(emotions)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "_ = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "y_prob = model.predict(x_test, batch_size=32, verbose=0)\n",
    "y_pred = [np.argmax(prob) for prob in y_prob]\n",
    "y_true = [np.argmax(true) for true in y_test]\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happy': 'ğŸ˜ƒ',\n",
       " 'surprise': 'ğŸ˜®',\n",
       " 'angry': 'ğŸ˜¡',\n",
       " 'neutral': 'ğŸ˜',\n",
       " 'disgust': 'ğŸ¤¢',\n",
       " 'fear': 'ğŸ˜±',\n",
       " 'sad': 'ğŸ˜¥'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dictonary which saves the basic emojis and allows for easy mapping\n",
    "emojiDict = {'happy': \"\\U0001F603\",\n",
    "             'surprise':\"\\U0001F62E\",\n",
    "             'angry':\"\\U0001F621\",\n",
    "             'neutral': \"\\U0001F610\",\n",
    "             'disgust':\"\\U0001F922\",\n",
    "             'fear':\"\\U0001F631\",\n",
    "             \"sad\":\"\\U0001F625\"}\n",
    "emojiDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ğŸ˜ƒ', 0.94), ('ğŸ˜¡', 0.02), ('ğŸ˜', 0.02), ('ğŸ˜¥', 0.02), ('ğŸ˜±', 0.01), ('ğŸ˜®', 0.0), ('ğŸ¤¢', 0.0)]                                                            "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fg/f4khpkkj039ffhb1np84vb7m0000gn/T/ipykernel_1669/763364434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preview\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# exit on ESC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#use open cv to get real time camera/image data\n",
    "\n",
    "cv2.namedWindow(\"preview\")\n",
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "if vc.isOpened(): # try to get the first frame\n",
    "    rval, frame = vc.read()\n",
    "else:\n",
    "    rval = False\n",
    "\n",
    "while rval:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \n",
    "    #get cropped image so it focuses on face\n",
    "    faces = face_cascade.detectMultiScale(frame, 1.1, 8)\n",
    "    if(len(faces) > 0):\n",
    "        (x, y, w, h) = faces[0]\n",
    "    else:\n",
    "        x, y, w, h = 0, 0, 50, 1\n",
    "    if(w > h):\n",
    "        w = h\n",
    "    else:\n",
    "        h = w\n",
    "    cropped_image = frame[y:y+h, x:x+w]\n",
    "    \n",
    "    #makes sure image is in correct format for model... reformatting\n",
    "    img = Image.fromarray(cropped_image , 'L')\n",
    "    img = img.resize((48, 48))\n",
    "    img = np.array(img.getdata())\n",
    "    img = img.reshape(1, 48, 48, 1)\n",
    "    img = img/255\n",
    "    \n",
    "    #run model and get probabilities for emoji\n",
    "    y_probs = model(img).numpy().flatten()\n",
    "    y_preds = np.flip(np.argsort(y_probs)).flatten()\n",
    "    y_preds = [(emojiDict[emotions[y_preds[i]]], np.round(y_probs.item(y_preds[i]), 2)) for i in range(len(emotions))]\n",
    "\n",
    "    print(f\"\\r{y_preds}\".ljust(150), end=\"\")\n",
    "    #print(y_probs)\n",
    "\n",
    "    cv2.imshow(\"preview\", cropped_image)\n",
    "    rval, frame = vc.read()\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27: # exit on ESC\n",
    "        break\n",
    "\n",
    "\n",
    "vc.release()\n",
    "cv2.destroyWindow(\"preview\")\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ğŸ¥º': [('ğŸ˜¥', 0.82),\n",
       "  ('ğŸ˜±', 0.1),\n",
       "  ('ğŸ˜', 0.05),\n",
       "  ('ğŸ˜¡', 0.02),\n",
       "  ('ğŸ˜ƒ', 0.01),\n",
       "  ('ğŸ¤¢', 0.0),\n",
       "  ('ğŸ˜®', 0.0)],\n",
       " 'ğŸ˜Ÿ': [('ğŸ˜¥', 0.38),\n",
       "  ('ğŸ˜±', 0.28),\n",
       "  ('ğŸ˜', 0.23),\n",
       "  ('ğŸ˜¡', 0.1),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ˜ƒ', 0.01),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜Š': [('ğŸ˜ƒ', 0.95),\n",
       "  ('ğŸ˜', 0.02),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ˜¥', 0.01),\n",
       "  ('ğŸ˜±', 0.0),\n",
       "  ('ğŸ˜¡', 0.0),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜­': [('ğŸ˜¥', 0.33),\n",
       "  ('ğŸ˜±', 0.25),\n",
       "  ('ğŸ˜ƒ', 0.13),\n",
       "  ('ğŸ˜', 0.12),\n",
       "  ('ğŸ˜¡', 0.1),\n",
       "  ('ğŸ˜®', 0.05),\n",
       "  ('ğŸ¤¢', 0.03)],\n",
       " 'ğŸ˜®': [('ğŸ˜±', 0.42),\n",
       "  ('ğŸ˜¡', 0.34),\n",
       "  ('ğŸ¤¢', 0.1),\n",
       "  ('ğŸ˜¥', 0.06),\n",
       "  ('ğŸ˜ƒ', 0.05),\n",
       "  ('ğŸ˜®', 0.02),\n",
       "  ('ğŸ˜', 0.01)],\n",
       " 'ğŸ˜¯': [('ğŸ˜®', 0.69),\n",
       "  ('ğŸ˜±', 0.29),\n",
       "  ('ğŸ˜¡', 0.01),\n",
       "  ('ğŸ˜¥', 0.01),\n",
       "  ('ğŸ˜ƒ', 0.0),\n",
       "  ('ğŸ˜', 0.0),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜«': [('ğŸ˜¥', 0.42),\n",
       "  ('ğŸ˜¡', 0.24),\n",
       "  ('ğŸ˜ƒ', 0.2),\n",
       "  ('ğŸ˜±', 0.1),\n",
       "  ('ğŸ˜', 0.02),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ¤¢', 0.01)],\n",
       " 'ğŸ˜¨': [('ğŸ˜¥', 0.54),\n",
       "  ('ğŸ˜', 0.19),\n",
       "  ('ğŸ˜±', 0.17),\n",
       "  ('ğŸ˜¡', 0.08),\n",
       "  ('ğŸ˜ƒ', 0.01),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜¤': [('ğŸ˜¡', 0.67),\n",
       "  ('ğŸ˜¥', 0.15),\n",
       "  ('ğŸ˜±', 0.07),\n",
       "  ('ğŸ¤¢', 0.05),\n",
       "  ('ğŸ˜', 0.04),\n",
       "  ('ğŸ˜ƒ', 0.01),\n",
       "  ('ğŸ˜®', 0.0)],\n",
       " 'ğŸ˜³': [('ğŸ˜±', 0.52),\n",
       "  ('ğŸ˜®', 0.42),\n",
       "  ('ğŸ˜¥', 0.02),\n",
       "  ('ğŸ˜', 0.02),\n",
       "  ('ğŸ˜¡', 0.01),\n",
       "  ('ğŸ˜ƒ', 0.01),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜²': [('ğŸ˜¥', 0.41),\n",
       "  ('ğŸ˜±', 0.3),\n",
       "  ('ğŸ˜¡', 0.15),\n",
       "  ('ğŸ¤¢', 0.1),\n",
       "  ('ğŸ˜', 0.02),\n",
       "  ('ğŸ˜ƒ', 0.02),\n",
       "  ('ğŸ˜®', 0.01)],\n",
       " 'ğŸ˜¥': [('ğŸ˜¥', 0.74),\n",
       "  ('ğŸ˜±', 0.11),\n",
       "  ('ğŸ˜', 0.08),\n",
       "  ('ğŸ˜¡', 0.04),\n",
       "  ('ğŸ˜ƒ', 0.02),\n",
       "  ('ğŸ¤¢', 0.01),\n",
       "  ('ğŸ˜®', 0.0)],\n",
       " 'ğŸ˜°': [('ğŸ˜¥', 0.37),\n",
       "  ('ğŸ˜±', 0.29),\n",
       "  ('ğŸ¤¢', 0.14),\n",
       "  ('ğŸ˜¡', 0.09),\n",
       "  ('ğŸ˜', 0.05),\n",
       "  ('ğŸ˜ƒ', 0.04),\n",
       "  ('ğŸ˜®', 0.01)],\n",
       " 'ğŸ˜±': [('ğŸ˜¡', 0.62),\n",
       "  ('ğŸ˜±', 0.24),\n",
       "  ('ğŸ˜ƒ', 0.04),\n",
       "  ('ğŸ˜¥', 0.04),\n",
       "  ('ğŸ¤¢', 0.03),\n",
       "  ('ğŸ˜®', 0.02),\n",
       "  ('ğŸ˜', 0.01)],\n",
       " 'ğŸ˜µ': [('ğŸ˜±', 0.44),\n",
       "  ('ğŸ˜¡', 0.36),\n",
       "  ('ğŸ˜¥', 0.07),\n",
       "  ('ğŸ˜®', 0.04),\n",
       "  ('ğŸ¤¢', 0.04),\n",
       "  ('ğŸ˜ƒ', 0.03),\n",
       "  ('ğŸ˜', 0.01)],\n",
       " 'ğŸ˜¶': [('ğŸ˜', 0.72),\n",
       "  ('ğŸ˜¥', 0.16),\n",
       "  ('ğŸ˜±', 0.06),\n",
       "  ('ğŸ˜¡', 0.04),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ˜ƒ', 0.01),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜ ': [('ğŸ˜¡', 0.71),\n",
       "  ('ğŸ˜¥', 0.11),\n",
       "  ('ğŸ˜', 0.09),\n",
       "  ('ğŸ˜±', 0.07),\n",
       "  ('ğŸ¤¢', 0.01),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ˜ƒ', 0.0)],\n",
       " 'ğŸ˜‘': [('ğŸ˜', 0.71),\n",
       "  ('ğŸ˜¥', 0.14),\n",
       "  ('ğŸ˜¡', 0.06),\n",
       "  ('ğŸ˜±', 0.05),\n",
       "  ('ğŸ˜ƒ', 0.03),\n",
       "  ('ğŸ˜®', 0.0),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜': [('ğŸ˜', 0.74),\n",
       "  ('ğŸ˜¥', 0.12),\n",
       "  ('ğŸ˜±', 0.06),\n",
       "  ('ğŸ˜¡', 0.06),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ˜ƒ', 0.01),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜„': [('ğŸ˜ƒ', 0.98),\n",
       "  ('ğŸ˜', 0.01),\n",
       "  ('ğŸ˜®', 0.0),\n",
       "  ('ğŸ˜¥', 0.0),\n",
       "  ('ğŸ˜¡', 0.0),\n",
       "  ('ğŸ˜±', 0.0),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ™': [('ğŸ˜', 0.63),\n",
       "  ('ğŸ˜¥', 0.15),\n",
       "  ('ğŸ˜¡', 0.1),\n",
       "  ('ğŸ˜±', 0.07),\n",
       "  ('ğŸ˜®', 0.02),\n",
       "  ('ğŸ˜ƒ', 0.02),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜€': [('ğŸ˜ƒ', 0.97),\n",
       "  ('ğŸ˜', 0.01),\n",
       "  ('ğŸ˜¥', 0.01),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ˜¡', 0.01),\n",
       "  ('ğŸ˜±', 0.0),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜': [('ğŸ˜ƒ', 0.48),\n",
       "  ('ğŸ˜', 0.33),\n",
       "  ('ğŸ˜¥', 0.08),\n",
       "  ('ğŸ˜±', 0.06),\n",
       "  ('ğŸ˜¡', 0.03),\n",
       "  ('ğŸ˜®', 0.02),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ¥´': [('ğŸ¤¢', 0.59),\n",
       "  ('ğŸ˜¥', 0.22),\n",
       "  ('ğŸ˜±', 0.1),\n",
       "  ('ğŸ˜¡', 0.06),\n",
       "  ('ğŸ˜', 0.02),\n",
       "  ('ğŸ˜ƒ', 0.01),\n",
       "  ('ğŸ˜®', 0.0)],\n",
       " 'ğŸ™‚': [('ğŸ˜ƒ', 0.97),\n",
       "  ('ğŸ˜', 0.01),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ˜¥', 0.0),\n",
       "  ('ğŸ˜¡', 0.0),\n",
       "  ('ğŸ˜±', 0.0),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜ƒ': [('ğŸ˜ƒ', 0.97),\n",
       "  ('ğŸ˜', 0.01),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ˜¥', 0.01),\n",
       "  ('ğŸ˜¡', 0.0),\n",
       "  ('ğŸ˜±', 0.0),\n",
       "  ('ğŸ¤¢', 0.0)],\n",
       " 'ğŸ˜•': [('ğŸ˜¥', 0.37),\n",
       "  ('ğŸ˜', 0.32),\n",
       "  ('ğŸ˜±', 0.16),\n",
       "  ('ğŸ˜¡', 0.12),\n",
       "  ('ğŸ˜ƒ', 0.01),\n",
       "  ('ğŸ˜®', 0.01),\n",
       "  ('ğŸ¤¢', 0.0)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#makes a dictionary for each emoji and their given values for each of the 7 emotions\n",
    "#does this by using model to analyze pre matched photos to their given emoji\n",
    "import os\n",
    "facePredict = {}\n",
    "directory = 'faces'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename[-3:]!=\"ore\":\n",
    "        f = os.path.join(directory, filename)\n",
    "        \n",
    "        img = Image.open(f)\n",
    "        img = np.array(img.getdata()).reshape(48, 48) / 255\n",
    "        img = np.expand_dims(img,0)\n",
    "        y_probs = model(img).numpy().flatten()\n",
    "        y_predss = np.flip(np.argsort(y_probs)).flatten()\n",
    "        y_predss = [(emojiDict[emotions[y_predss[i]]], np.round(y_probs.item(y_predss[i]), 2)) for i in range(len(emotions))]\n",
    "    \n",
    "        facePredict[filename[:-4]] =y_predss\n",
    "       \n",
    "facePredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fg/f4khpkkj039ffhb1np84vb7m0000gn/T/ipykernel_1669/2697306028.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#print(y_preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#print(facePredict['ğŸ˜'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfacePredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ğŸ˜'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_preds' is not defined"
     ]
    }
   ],
   "source": [
    "#calculates how close the expression is to an emoji\n",
    "def dist(one, two):\n",
    "    l = ['ğŸ˜¥','ğŸ˜±','ğŸ˜','ğŸ˜¡','ğŸ˜ƒ','ğŸ¤¢','ğŸ˜®']\n",
    "    value = 0\n",
    "    for i in range(7):\n",
    "        val = 0\n",
    "        \n",
    "        if(one[i][0] == l[i]):\n",
    "            val+=one[i][1]\n",
    "        if(two[i][0] == l[i]):\n",
    "            val-=two[i][1]\n",
    "        val = val*val\n",
    "        value+=val\n",
    "    return math.sqrt(value)           \n",
    "#print(y_preds)\n",
    "#print(facePredict['ğŸ˜'])\n",
    "dist(y_preds,facePredict['ğŸ˜'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fg/f4khpkkj039ffhb1np84vb7m0000gn/T/ipykernel_1669/2991342966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdictt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfacePredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdistt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdictt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdistt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dist' is not defined"
     ]
    }
   ],
   "source": [
    "#goes through each emoji and sees how close it is... \n",
    "#returns the 3 most likely emoji to match expression\n",
    "dictt = {}\n",
    "for item in facePredict.items():\n",
    "    distt = dist(y_preds, item[1])\n",
    "    dictt[item[0]]= distt\n",
    "  \n",
    "sorted_dicdata = sorted(dictt.items(), key=operator.itemgetter(1),    reverse=True)\n",
    "    \n",
    "\n",
    "sorted_dicdata[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ğŸ˜', 0.02), ('ğŸ™‚', 0.02), ('ğŸ˜ƒ', 0.02)]                                                                                                              "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fg/f4khpkkj039ffhb1np84vb7m0000gn/T/ipykernel_1669/4051346837.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preview\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# exit on ESC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#FINAL.... RETURNS 3 EMOJIS REAL TIME\n",
    "\n",
    "cv2.namedWindow(\"preview\")\n",
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "if vc.isOpened(): # try to get the first frame\n",
    "    rval, frame = vc.read()\n",
    "else:\n",
    "    rval = False\n",
    "\n",
    "while rval:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \n",
    "    #get cropped image so it focuses on face\n",
    "    faces = face_cascade.detectMultiScale(frame, 1.1, 8)\n",
    "    if(len(faces) > 0):\n",
    "        (x, y, w, h) = faces[0]\n",
    "    else:\n",
    "        x, y, w, h = 0, 0, 50, 1\n",
    "    if(w > h):\n",
    "        w = h\n",
    "    else:\n",
    "        h = w\n",
    "    cropped_image = frame[y:y+h, x:x+w]\n",
    "    \n",
    "    #makes sure image is in correct format for model... reformatting\n",
    "    img = Image.fromarray(cropped_image , 'L')\n",
    "    img = img.resize((48, 48))\n",
    "    img = np.array(img.getdata())\n",
    "    img = img.reshape(1, 48, 48, 1)\n",
    "    img = img/255\n",
    "    \n",
    "    #run model and get probabilities for emoji\n",
    "    y_probs = model(img).numpy().flatten()\n",
    "    y_pred = np.flip(np.argsort(y_probs)).flatten()\n",
    "    y_pred = [(emojiDict[emotions[y_pred[i]]], np.round(y_probs.item(y_pred[i]), 2)) for i in range(len(emotions))]\n",
    "\n",
    "    #print(f\"\\r{y_preds}\".ljust(150), end=\"\")\n",
    "    dicttt = {}\n",
    "    for item in facePredict.items():\n",
    "        disttt = dist(y_pred, item[1])\n",
    "        dicttt[item[0]]= disttt\n",
    "  \n",
    "    sorted_dic = sorted(dicttt.items(), key=operator.itemgetter(1),    reverse=True)\n",
    " \n",
    "    print(f\"\\r{sorted_dic[-3:]}\".ljust(150), end=\"\")\n",
    "\n",
    "   \n",
    "\n",
    "    cv2.imshow(\"preview\", cropped_image)\n",
    "    rval, frame = vc.read()\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27: # exit on ESC\n",
    "        break\n",
    "\n",
    "\n",
    "vc.release()\n",
    "cv2.destroyWindow(\"preview\")\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
